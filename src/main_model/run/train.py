import os
import sys
import logging
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
import nltk
import datasets
from datasets import load_dataset, load_metric
from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    HfArgumentParser,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    set_seed,
)
import transformers
from filelock import FileLock
from transformers.utils import check_min_version, is_offline_mode
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils.versions import require_version


def get_parameter_count(model):
    return sum(param.numel() for param in model.parameters() if param.requires_grad)

check_min_version("4.9.0.dev0")
require_version("datasets>=1.8.0", "Please install datasets>=1.8.0")

logger = logging.getLogger(__name__)

try:
    nltk.data.find("tokenizers/punkt")
except (LookupError, OSError):
    if is_offline_mode():
        raise LookupError("Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files")
    with FileLock(".lock") as lock:
        nltk.download("punkt", quiet=True)

@dataclass
class ModelConfig:
    """
    Configuration for model, tokenizer, and config files.
    """
    model_name_or_path: str = field(
        metadata={"help": "Path to pre-trained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None,
        metadata={"help": "Name or path to the pre-trained config if different from model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None,
        metadata={"help": "Name or path to the pre-trained tokenizer if different from model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Directory to store the pre-trained models downloaded from huggingface.co"}
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use a fast tokenizer backed by the tokenizers library"}
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "Specific model version to use (branch name, tag name or commit id)"}
    )
    use_auth_token: bool = field(
        default=False,
        metadata={"help": "Use the token generated by `transformers-cli login` (necessary for private models)"}
    )

@dataclass
class DataConfig:
    """
    Configuration for data inputs for training and evaluation.
    """
    dataset_name: Optional[str] = field(
        default=None,
        metadata={"help": "Name of the dataset to use from the datasets library"}
    )
    dataset_config_name: Optional[str] = field(
        default=None,
        metadata={"help": "Configuration name of the dataset from the datasets library"}
    )
    text_column: Optional[str] = field(
        default=None,
        metadata={"help": "Name of the column containing the full texts"}
    )
    summary_column: Optional[str] = field(
        default=None,
        metadata={"help": "Name of the column containing the summaries"}
    )
    train_file: Optional[str] = field(
        default=None,
        metadata={"help": "Path to the training data file (csv or json)"}
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "Optional path to the evaluation data file (csv or json)"}
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "Optional path to the test data file (csv or json)"}
    )
    overwrite_cache: bool = field(
        default=False,
        metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes for data preprocessing"}
    )
    max_source_length: Optional[int] = field(
        default=1024,
        metadata={"help": "Maximum total input sequence length after tokenization"}
    )
    max_target_length: Optional[int] = field(
        default=128,
        metadata={"help": "Maximum total target sequence length after tokenization"}
    )
    val_max_target_length: Optional[int] = field(
        default=None,
        metadata={"help": "Maximum total target sequence length for validation after tokenization"}
    )
    pad_to_max_length: bool = field(
        default=False,
        metadata={"help": "Pad all samples to max sentence length"}
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={"help": "For debugging, truncate the number of training examples"}
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={"help": "For debugging, truncate the number of evaluation examples"}
    )
    max_predict_samples: Optional[int] = field(
        default=None,
        metadata={"help": "For debugging, truncate the number of prediction examples"}
    )
    num_beams: Optional[int] = field(
        default=None,
        metadata={"help": "Number of beams to use for evaluation"}
    )
    ignore_pad_token_for_loss: bool = field(
        default=True,
        metadata={"help": "Ignore tokens corresponding to padded labels in loss computation"}
    )
    source_prefix: Optional[str] = field(
        default=None,
        metadata={"help": "Prefix to add before every source text (useful for T5 models)"}
    )

    def __post_init__(self):
        if self.dataset_name is None and self.train_file is None and self.validation_file is None:
            raise ValueError("Need either a dataset name or training/validation file.")
        else:
            if self.train_file is not None:
                ext = self.train_file.split(".")[-1]
                assert ext in ["csv", "json"], "`train_file` should be a csv or json file."
            if self.validation_file is not None:
                ext = self.validation_file.split(".")[-1]
                assert ext in ["csv", "json"], "`validation_file` should be a csv or json file."
        if self.val_max_target_length is None:
            self.val_max_target_length = self.max_target_length

# Mapping for summarization datasets
summarization_datasets = {
    "amazon_reviews_multi": ("review_body", "review_title"),
    "big_patent": ("description", "abstract"),
    "cnn_dailymail": ("article", "highlights"),
    "orange_sum": ("text", "summary"),
    "pn_summary": ("article", "summary"),
    "psc": ("extract_text", "summary_text"),
    "samsum": ("dialogue", "summary"),
    "thaisum": ("body", "summary"),
    "xglue": ("news_body", "news_title"),
    "xsum": ("document", "summary"),
    "wiki_summary": ("article", "highlights"),
}

def main():
    # Parse arguments
    parser = HfArgumentParser((ModelConfig, DataConfig, Seq2SeqTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y/%m/%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log training parameters
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, "
        f"n_gpu: {training_args.n_gpu}, distributed training: {training_args.local_rank != -1}, "
        f"16-bits training: {training_args.fp16}"
    )
    logger.info(f"Training/evaluation parameters {training_args}")

    if data_args.source_prefix is None and model_args.model_name_or_path.startswith("t5"):
        logger.warning(
            "You're using a T5 model but didn't provide a source prefix. "
            "It's recommended to use a prefix, e.g., `--source_prefix 'summarize: '`"
        )

    # Detect last checkpoint
    last_checkpoint = None
    if (
        os.path.isdir(training_args.output_dir)
        and training_args.do_train
        and not training_args.overwrite_output_dir
    ):
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) exists and is not empty. "
                "Use --overwrite_output_dir to proceed."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Resuming training from checkpoint {last_checkpoint}. "
                "To train from scratch, change `--output_dir` or add `--overwrite_output_dir`."
            )

    # Set seed
    set_seed(training_args.seed)

    # Load datasets
    if data_args.dataset_name is not None:
        raw_datasets = load_dataset(
            data_args.dataset_name,
            data_args.dataset_config_name,
            cache_dir=model_args.cache_dir
        )
    else:
        data_files = {}
        if data_args.train_file is not None:
            data_files["train"] = data_args.train_file
            ext = data_args.train_file.split(".")[-1]
        if data_args.validation_file is not None:
            data_files["validation"] = data_args.validation_file
            ext = data_args.validation_file.split(".")[-1]
        if data_args.test_file is not None:
            data_files["test"] = data_args.test_file
            ext = data_args.test_file.split(".")[-1]
        raw_datasets = load_dataset(ext, data_files=data_files, cache_dir=model_args.cache_dir)

    # Load model and tokenizer
    config = AutoConfig.from_pretrained(
        model_args.config_name or model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=model_args.use_auth_token,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name or model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast_tokenizer,
        revision=model_args.model_revision,
        use_auth_token=model_args.use_auth_token,
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_args.model_name_or_path,
        from_tf=".ckpt" in model_args.model_name_or_path,
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=model_args.use_auth_token,
    )

    logger.info(f'The model has {get_parameter_count(model):,} trainable parameters')

    model.resize_token_embeddings(len(tokenizer))

    if model.config.decoder_start_token_id is None:
        raise ValueError("Ensure that `config.decoder_start_token_id` is correctly defined")

    # Set prefix if using T5
    prefix = data_args.source_prefix if data_args.source_prefix is not None else ""

    # Get column names
    if training_args.do_train:
        column_names = raw_datasets["train"].column_names
    elif training_args.do_eval:
        column_names = raw_datasets["validation"].column_names
    elif training_args.do_predict:
        column_names = raw_datasets["test"].column_names
    else:
        logger.info("Nothing to do. Please set `do_train`, `do_eval` and/or `do_predict`.")
        return

    # Get text and summary columns
    dataset_columns = summarization_datasets.get(data_args.dataset_name, None)
    if data_args.text_column is None:
        text_column = dataset_columns[0] if dataset_columns else column_names[0]
    else:
        text_column = data_args.text_column
        if text_column not in column_names:
            raise ValueError(f"Text column '{text_column}' not found in dataset columns {column_names}")

    if data_args.summary_column is None:
        summary_column = dataset_columns[1] if dataset_columns else column_names[1]
    else:
        summary_column = data_args.summary_column
        if summary_column not in column_names:
            raise ValueError(f"Summary column '{summary_column}' not found in dataset columns {column_names}")

    # Temporarily set max target length for training
    max_target_length = data_args.max_target_length
    padding_strategy = "max_length" if data_args.pad_to_max_length else False

    if training_args.label_smoothing_factor > 0 and not hasattr(model, "prepare_decoder_input_ids_from_labels"):
        logger.warning(
            f"Label smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for "
            f"`{model.__class__.__name__}`. This may lead to loss being calculated twice and increased memory usage."
        )

    def preprocess_function(examples):
        inputs = examples[text_column]
        targets = examples[summary_column]
        inputs = [prefix + inp for inp in inputs]
        model_inputs = tokenizer(
            inputs,
            max_length=data_args.max_source_length,
            padding=padding_strategy,
            truncation=True
        )

        with tokenizer.as_target_tokenizer():
            labels = tokenizer(
                targets,
                max_length=max_target_length,
                padding=padding_strategy,
                truncation=True
            )

        # Replace pad token id with -100 if needed
        if padding_strategy == "max_length" and data_args.ignore_pad_token_for_loss:
            labels["input_ids"] = [
                [(label if label != tokenizer.pad_token_id else -100) for label in label_ids]
                for label_ids in labels["input_ids"]
            ]

        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    if training_args.do_train:
        if "train" not in raw_datasets:
            raise ValueError("Training requires a train dataset")
        train_dataset = raw_datasets["train"]
        if data_args.max_train_samples:
            train_dataset = train_dataset.select(range(data_args.max_train_samples))
        with training_args.main_process_first(desc="Processing train dataset"):
            train_dataset = train_dataset.map(
                preprocess_function,
                batched=True,
                num_proc=data_args.preprocessing_num_workers,
                remove_columns=column_names,
                load_from_cache_file=not data_args.overwrite_cache,
                desc="Tokenizing train dataset",
            )

    if training_args.do_eval:
        max_target_length = data_args.val_max_target_length
        if "validation" not in raw_datasets:
            raise ValueError("Evaluation requires a validation dataset")
        eval_dataset = raw_datasets["validation"]
        if data_args.max_eval_samples:
            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))
        with training_args.main_process_first(desc="Processing validation dataset"):
            eval_dataset = eval_dataset.map(
                preprocess_function,
                batched=True,
                num_proc=data_args.preprocessing_num_workers,
                remove_columns=column_names,
                load_from_cache_file=not data_args.overwrite_cache,
                desc="Tokenizing validation dataset",
            )

    if training_args.do_predict:
        max_target_length = data_args.val_max_target_length
        if "test" not in raw_datasets:
            raise ValueError("Prediction requires a test dataset")
        predict_dataset = raw_datasets["test"]
        if data_args.max_predict_samples:
            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))
        with training_args.main_process_first(desc="Processing test dataset"):
            predict_dataset = predict_dataset.map(
                preprocess_function,
                batched=True,
                num_proc=data_args.preprocessing_num_workers,
                remove_columns=column_names,
                load_from_cache_file=not data_args.overwrite_cache,
                desc="Tokenizing test dataset",
            )

    # Data collator
    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        label_pad_token_id=label_pad_token_id,
        pad_to_multiple_of=8 if training_args.fp16 else None,
    )

    metric = load_metric("rouge")

    def postprocess_text(predictions, references):
        predictions = [pred.strip() for pred in predictions]
        references = [ref.strip() for ref in references]

        # Rouge expects newline after each sentence
        predictions = ["\n".join(nltk.sent_tokenize(pred)) for pred in predictions]
        references = ["\n".join(nltk.sent_tokenize(ref)) for ref in references]

        return predictions, references

    def compute_metrics(eval_preds):
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
        if data_args.ignore_pad_token_for_loss:
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}

        prediction_lengths = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
        result["gen_len"] = np.mean(prediction_lengths)
        result = {k: round(v, 4) for k, v in result.items()}
        return result

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics if training_args.predict_with_generate else None,
    )

    # Training
    if training_args.do_train:
        resume_checkpoint = None
        if training_args.resume_from_checkpoint:
            resume_checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            resume_checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)
        trainer.save_model()  # Save model and tokenizer

        metrics = train_result.metrics
        max_train_samples = data_args.max_train_samples or len(train_dataset)
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()

    # Evaluation
    if training_args.do_eval:
        logger.info("*** Evaluate ***")

        eval_metrics = trainer.evaluate(
            max_length=data_args.val_max_target_length,
            num_beams=data_args.num_beams,
            metric_key_prefix="eval"
        )
        max_eval_samples = data_args.max_eval_samples or len(eval_dataset)
        eval_metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

        trainer.log_metrics("eval", eval_metrics)
        trainer.save_metrics("eval", eval_metrics)

    # Prediction
    if training_args.do_predict:
        logger.info("*** Predict ***")

        predict_results = trainer.predict(
            test_dataset=predict_dataset,
            max_length=data_args.val_max_target_length,
            num_beams=data_args.num_beams,
            metric_key_prefix="predict"
        )
        predict_metrics = predict_results.metrics
        max_predict_samples = data_args.max_predict_samples or len(predict_dataset)
        predict_metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

        trainer.log_metrics("predict", predict_metrics)
        trainer.save_metrics("predict", predict_metrics)

        if trainer.is_world_process_zero():
            if training_args.predict_with_generate:
                predictions = tokenizer.batch_decode(
                    predict_results.predictions,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                predictions = [pred.strip() for pred in predictions]
                output_prediction_file = os.path.join(training_args.output_dir, "generated_predictions.txt")
                with open(output_prediction_file, "w") as writer:
                    writer.write("\n".join(predictions))

def _mp_fn(index):
    main()

if __name__ == "__main__":
    main()
